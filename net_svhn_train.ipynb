{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/domiurg/ML_class_svhn\"\n",
    "!cat ML_class_svhn/svhn-part* >> svhn.tar.gz\n",
    "!tar -xvzf svhn.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import random_rotation, random_shear, random_shift, random_zoom\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, TensorBoard\n",
    "from os.path import join\n",
    "from os import makedirs\n",
    "import scipy.io as sio\n",
    "import time\n",
    "from numpy import savez_compressed, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function peforms various data augmentation techniques to the dataset\n",
    "    \n",
    "    @parameters:\n",
    "        dataset: the feature training dataset in numpy array with shape [num_examples, num_rows, num_cols, num_channels] (since it is an image in numpy array)\n",
    "        dataset_labels: the corresponding training labels of the feature training dataset in the same order, and numpy array with shape [num_examples, <anything>]\n",
    "        augmentation_factor: how many times to perform augmentation.\n",
    "        use_random_rotation: whether to use random rotation. default: true\n",
    "        use_random_shift: whether to use random shift. default: true\n",
    "        use_random_shear: whether to use random shear. default: true\n",
    "        use_random_zoom: whether to use random zoom. default: true\n",
    "        \n",
    "    @returns:\n",
    "        augmented_image: augmented dataset\n",
    "        augmented_image_labels: labels corresponding to augmented dataset in order.\n",
    "'''\n",
    "def augment_data(dataset, dataset_labels, augementation_factor=1,\n",
    "                 use_random_rotation=True, \n",
    "                 use_random_shear=True, \n",
    "                 use_random_shift=True, \n",
    "                 use_random_zoom=True):\n",
    "\taugmented_image = []\n",
    "\taugmented_image_labels = []\n",
    "\n",
    "\tfor num in range (0, dataset.shape[0]):\n",
    "\n",
    "\t\tfor i in range(0, augementation_factor):\n",
    "\t\t\t# original image:\n",
    "\t\t\taugmented_image.append(dataset[num])\n",
    "\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "\t\t\tif use_random_rotation:\n",
    "\t\t\t\taugmented_image.append(random_rotation(dataset[num], 40, row_axis=0, col_axis=1, channel_axis=2))\n",
    "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "\t\t\tif use_random_shear:\n",
    "\t\t\t\taugmented_image.append(random_shear(dataset[num], 0.3, row_axis=0, col_axis=1, channel_axis=2))\n",
    "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "\t\t\tif use_random_shift:\n",
    "\t\t\t\taugmented_image.append(random_shift(dataset[num], 0.15, 0.15, row_axis=0, col_axis=1, channel_axis=2))\n",
    "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "\t\t\tif use_random_zoom:\n",
    "\t\t\t\taugmented_image.append(random_zoom(dataset[num], (0.8, 0.8), row_axis=0, col_axis=1, channel_axis=2))\n",
    "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "\treturn np.array(augmented_image), np.array(augmented_image_labels)\n",
    "\n",
    "data_dir = 'svhn'\n",
    "train_fn = 'train_32x32.mat'\n",
    "test_fn = 'test_32x32.mat'\n",
    "#  train_ex_fn = 'extra_32x32.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loaded = load('svhn_aug4.npz')\n",
    "X_train = loaded['X_train']\n",
    "X_test = loaded['X_test']\n",
    "y_train = loaded['y_train']\n",
    "y_test = loaded['y_test']\n",
    "\n",
    "el = time.time() - start\n",
    "print('Elapsed: {} s'.format(el))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = sio.loadmat(join(data_dir, train_fn))\n",
    "# # train_ex = sio.loadmat(join(data_dir, train_ex_fn))\n",
    "# test_data = sio.loadmat(join(data_dir, test_fn))\n",
    "\n",
    "# X_train = train_data['X']\n",
    "# y_train = train_data['y']\n",
    "\n",
    "# # X_extra = train_ex['X']\n",
    "# # y_extra = train_ex['y']\n",
    "\n",
    "# X_test = test_data['X']\n",
    "# y_test = test_data['y']\n",
    "\n",
    "# X_train = X_train.transpose(3, 0, 1, 2)\n",
    "# # X_extra = X_extra.transpose(3, 0, 1, 2)\n",
    "# # X_train = np.concatenate((X_train, X_extra))\n",
    "# X_test = X_test.transpose(3, 0, 1, 2)\n",
    "\n",
    "# # y_train = np.concatenate((y_train, y_extra))\n",
    "# y_train = y_train.reshape(y_train.shape[0])\n",
    "# y_test = y_test.reshape(y_test.shape[0])\n",
    "\n",
    "# y_train[y_train == 10] = 0\n",
    "# y_test[y_test == 10] = 0\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# X_train, y_train = augment_data(X_train, y_train, augementation_factor=2, \n",
    "#                                 use_random_shift=False, use_random_shear=False)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# #one-hot encode target column\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# savez_compressed('svhn_aug4.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 304005\n",
    "t = str(y_train[i])\n",
    "plt.imshow(X_train[i])\n",
    "plt.title(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 5, 5\n",
    "start = 304005\n",
    "# Magic line for adequate graph size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "fig, axes = plt.subplots(nrows=r, ncols=c)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for ax, img, label in zip(axes.flatten(), X_train[start:start+r*c], y_train[start:start + r*c]):\n",
    "    ax.imshow(img)\n",
    "    ax.set(title = str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation\n",
    "model = 0\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(32, 32, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(2, 2), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, kernel_size=(2, 2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, kernel_size=(1, 1), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "filepath = \"svhn-best_19.03.2020.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=30)\n",
    "csv_history = 'svhn-history_19.03.2020.csv'\n",
    "csv_logger = CSVLogger(csv_history, append=False)\n",
    "\n",
    "callbacks = [checkpoint, early_stop, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=2048,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=350, verbose=1,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('svhn-final_17.03.2020.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
